{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch3_예제_12_Softmax_Classification_NumPy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KwonYN/TIL/blob/master/PYTHON/TENSORFLOW/Ch3_%EC%98%88%EC%A0%9C_12_Softmax_Classification_NumPy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1qdcIH5hD8T",
        "colab_type": "text"
      },
      "source": [
        "# Chapter3-4. Deep Learning 기초 : Softmax Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jtCR3YxhOdM",
        "colab_type": "text"
      },
      "source": [
        ">## [예제3-12] Softmax Classification (Python)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A_LlSkehcbI",
        "colab_type": "text"
      },
      "source": [
        ">### Load modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJWxIPrVg1Q7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "338ab80e-4efe-4ba1-93c6-9300874bc529"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Module Loaded.\")\n",
        "print(\"NumPy Version :{}\".format(np.__version__))\n",
        "print(\"Matplotlib Version :{}\".format(plt.matplotlib.__version__))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Module Loaded.\n",
            "NumPy Version :1.17.4\n",
            "Matplotlib Version :3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bypsRBWbhgYJ",
        "colab_type": "text"
      },
      "source": [
        "> ### Input and Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NnanrW5heqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_input = np.array([  [1, 1],  [2, 2.5], [2.5, 1.3], [4.3, 9.5], [5.5, 7.0], [6, 8.2],   [7, 5],     [8, 6],   [9, 4.5] ], dtype=np.float)\n",
        "if 0:\n",
        "  labels = np.array([ [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0],  [0, 1, 0],  [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1] ], dtype=np.float)\n",
        "else:\n",
        "  labels_tmp = np.array( [0, 0, 0, 1, 1, 1, 2, 2, 2] )  # 실제 마지막에 np.argmax( ? )의 결과로 나와야 하는 것들이 이렇게 배열로 주어졌다면?\n",
        "  labels = np.eye(3)[labels_tmp]  # one-hot encoding의 형태로 만들어줌!\n",
        "\n",
        "#print(x_input)\n",
        "#print(x_input.reshape(-1, 1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVaDdfQAhv6H",
        "colab_type": "text"
      },
      "source": [
        ">### Activation Function : Softmax Function\n",
        ">$S(y_{i}) = \\frac{e^{y_{i}}}{\\sum_{j=0}^{m-1}e^{y_{j}}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A5xDDTHiZLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Softmax(z: np.ndarray)->np.ndarray: # 위의 수식인 거\n",
        "    c = np.max(z, axis=1) # 한 행에서의 최대값\n",
        "    c = c.reshape((-1, 1))# n행 1열로 만들어줌!!\n",
        "                          # 그리고 바로 밑의 행에서 z - c를 해줄 때, z의 사이즈와 shape에 맞추어 스트레치될 것임!! ( n행 3열로! )\n",
        "\n",
        "    exp_z = np.exp(z - c) # np.exp(0) = 1.0 - 지수가 커지게 되면, 그 값이 exponentially 증가할 수 있음.\n",
        "                          # 그래서 최대값으로 빼주고, e^0을 해줌으로써 1로 만들어주는 것! 즉, Overflow의 가능성을 확 줄여주기 위함!\n",
        "\n",
        "    sum_exp_z = np.sum(exp_z, axis=1)\n",
        "    sum_exp_z = sum_exp_z.reshape((-1, 1))\n",
        "    y = exp_z / sum_exp_z\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMCRwn60iZuA",
        "colab_type": "text"
      },
      "source": [
        ">## Hypothesis\n",
        ">## $H(X) = S(XW+B)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTGXWI41hkDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_var = 2\n",
        "n_class = 3\n",
        "\n",
        "w = np.random.normal(size=(n_var, n_class))\n",
        "b = np.random.normal(size=(n_class,))\n",
        "\n",
        "def Hypothesis(x: np.ndarray) -> np.ndarray:\n",
        "    logits = np.matmul(x, w) + b\n",
        "    return Softmax(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnPxYOxUizpI",
        "colab_type": "text"
      },
      "source": [
        ">## Cost Function : Cross Entropy Error\n",
        ">## $cost(W,b) = -\\sum_{i=1}^{m}p_{i}\\log_{2}q_{i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DReN23b8itqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Cost(x: np.ndarray, t: np.ndarray):  # Cross Entropy Error : 이전의 logistic regression(0이냐 1이냐 classify)와 다르게...\n",
        "                                         #                      ?\n",
        "    return np.mean(-np.sum(t * np.log(Hypothesis(x))))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh_7Y4J4jfKM",
        "colab_type": "text"
      },
      "source": [
        ">### Gradient\n",
        ">### $\\frac{d}{dx}f(x) = \\lim_{\\delta = 0} \\frac{f(x+\\delta) - f(x-\\delta)}{2\\delta}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWFxHR5Djfp7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3187e90a-8c0b-4e37-e183-cc35eb476fa1"
      },
      "source": [
        "# 기울기(Gradient) : 수치 미분\n",
        "print(w.shape[0])\n",
        "\n",
        "def Gradient(x: int or np.ndarray, t: int or np.ndarray)->tuple:\n",
        "    global w, b\n",
        "    pres_w = w.copy()\n",
        "    grad_w = np.zeros_like(w)\n",
        "    delta = 1e-7\n",
        "    for idx_r in range(w.shape[0]):\n",
        "        for idx_c in range(w.shape[1]):\n",
        "            w[idx_r, idx_c] = pres_w[idx_r, idx_c] + delta\n",
        "            cost_p = Cost(x, t)\n",
        "            w[idx_r, idx_c] = pres_w[idx_r, idx_c] - delta\n",
        "            cost_m = Cost(x, t)\n",
        "            grad_w[idx_r, idx_c] = (cost_p-cost_m)/(2*delta)\n",
        "            w[idx_r, idx_c] = pres_w[idx_r, idx_c]\n",
        "\n",
        "    pres_b = b.copy()\n",
        "    grad_b = np.zeros_like(b)\n",
        "    for idx in range(b.size):\n",
        "        b[idx] = pres_b[idx] + delta\n",
        "        cost_p = Cost(x, t)\n",
        "        b[idx] = pres_b[idx] - delta\n",
        "        cost_m = Cost(x, t)\n",
        "        grad_b[idx] = (cost_p-cost_m)/(2*delta)\n",
        "        b[idx] = pres_b[idx]\n",
        "\n",
        "    return grad_w, grad_b"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CKEPUgrjtiD",
        "colab_type": "text"
      },
      "source": [
        ">## Training 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ0q8BMLjtGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fbdfb646-02a6-42f4-e593-58b6307543fd"
      },
      "source": [
        "N_training = 1000000\n",
        "training_idx = np.arange(0, N_training+1, 1)\n",
        "cost_graph = np.zeros(N_training+1)\n",
        "\n",
        "cost_graph[0] = Cost(x_input, labels)\n",
        "print(\"[{:>5}] cost = {:>10.4}\".format(0, cost_graph[0]))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    0] cost =      28.56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tTZWVJ2kOqp",
        "colab_type": "text"
      },
      "source": [
        ">## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE77IEJLjouY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d9f46eaf-af78-4fe4-fab0-5c29f1913910"
      },
      "source": [
        "# 학습 (Training)\n",
        "learning_rate = 0.01\n",
        "for cnt_training in range(1, N_training+1):\n",
        "    cost_graph[cnt_training] = Cost(x_input, labels)\n",
        "    grad_w, grad_b = Gradient(x_input, labels)\n",
        "    w -= learning_rate * grad_w\n",
        "    b -= learning_rate * grad_b\n",
        "    if cnt_training % 1000 == 0:\n",
        "        print(\"[{:>5}] cost = {:>10.4}\".format(cnt_training, cost_graph[cnt_training]))\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1000] cost =     0.3152\n",
            "[ 2000] cost =     0.1566\n",
            "[ 3000] cost =     0.1047\n",
            "[ 4000] cost =    0.07876\n",
            "[ 5000] cost =    0.06322\n",
            "[ 6000] cost =    0.05284\n",
            "[ 7000] cost =    0.04542\n",
            "[ 8000] cost =    0.03984\n",
            "[ 9000] cost =    0.03549\n",
            "[10000] cost =    0.03201\n",
            "[0 0 0 1 1 1 2 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SdfjjqHkN6u",
        "colab_type": "text"
      },
      "source": [
        ">## Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im_PaXBtkOHQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "617256b3-151c-419b-d8c8-e147cdb601d4"
      },
      "source": [
        "print(np.argmax(Hypothesis(x_input), axis=1))\n",
        "\n",
        "\n",
        "# Training 상황에 대한 그래프 출력\n",
        "# 1)Training 회수 별 Cost 값\n",
        "fig_cost, ax_cost = plt.subplots()\n",
        "ax_cost.plot(training_idx, cost_graph)\n",
        "ax_cost.set_title(\"'Cost / Training Count' Graph\")\n",
        "ax_cost.set_xlabel(\"Train Count\")\n",
        "ax_cost.set_ylabel(\"Cost\")\n",
        "ax_cost.grid(True)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# 학습 잘 되었는지 볼까?\n",
        "print(np.argmax(Hypothesis([[0.1, 3.1]]), axis=1))\n",
        "print(np.argmax(Hypothesis([[4.7, 10.0]]), axis=1))\n",
        "print(np.argmax(Hypothesis([[9, 3.1]]), axis=1))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 1 1 1 2 2 2]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEWCAYAAACOv5f1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0]\n",
            "[1]\n",
            "[2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NwHRclUchzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}