{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch3_예제_08_Multi_Variable_LR_NumPy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KwonYN/TIL/blob/master/PYTHON/TENSORFLOW/Ch3_%EC%98%88%EC%A0%9C_08_Multi_Variable_LR_NumPy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9k7wggIGAAg",
        "colab_type": "text"
      },
      "source": [
        "# Chapter3-2. Deep Learning 기초 : Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGs0O-36GCbv",
        "colab_type": "text"
      },
      "source": [
        ">## [예제3-8] Multi Variable Linear Regression (NumPy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yuA5REJG2EP",
        "colab_type": "text"
      },
      "source": [
        ">### Load modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV37V0ugF6VI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "dbb36177-057b-4ed8-e872-6c850dffc3bf"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Module Loaded.\")\n",
        "print(\"NumPy Version :{}\".format(np.__version__))\n",
        "print(\"Matplotlib Version :{}\".format(plt.matplotlib.__version__))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Module Loaded.\n",
            "NumPy Version :1.17.4\n",
            "Matplotlib Version :3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_dUEZY0G6xQ",
        "colab_type": "text"
      },
      "source": [
        "> ### Input and Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtBf0ENWGA1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Weight, Height, Smoking vs Life-span\n",
        "x_input = np.array([[60., 167.64, 1.], [65., 152.4, 0.], [55., 182.88, 1.] ]) #, [85, 183.4, 1.]])\n",
        "labels = np.array([[66], [74], [78] ]) #, [72]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zsvH8foHCFj",
        "colab_type": "text"
      },
      "source": [
        ">### Hypothesis : Linear Equation (Multi-Variable)\n",
        ">### $X = \\begin{bmatrix} x_{11} & x_{12} & x_{13} \\\\ x_{21} & x_{22} & x_{23} \\\\ x_{31} & x_{32} & x_{33} \\\\ x_{41} & x_{42} & x_{43}\\end{bmatrix}$,  $W = \\begin{bmatrix} w_{1} \\\\ w_{2} \\\\w_{3}\\end{bmatrix}$\n",
        ">### $H(x) = XW + b$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O97LZjLUHflt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Weight, Bias\n",
        "W = np.random.normal(size=(3, 1))\n",
        "b = np.random.normal()\n",
        "\n",
        "# Hypothesis\n",
        "def Hypothesis(X: float or np.ndarray)->float:\n",
        "    return np.matmul(X, W) + b\n",
        "    # return np.dot(X, W) + b   # 동일한 결과가 나옴!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S-GdYr9JNXh",
        "colab_type": "text"
      },
      "source": [
        ">### Cost Function : Mean Squared Error (MSE)\n",
        ">### $cost(W,b) = \\sum_{i=1}^{n}(h(x_{i})-y_{i})^{2}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXhLS4dDJhko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cost function\n",
        "def Cost(x: float or np.ndarray, t: float or np.ndarray)->float:\n",
        "    # return np.mean((Hypothesis(x) - t)**2)\n",
        "    return np.mean(np.square(Hypothesis(x) - t))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBdsFzVGJhQU",
        "colab_type": "text"
      },
      "source": [
        ">### Gradient\n",
        ">### $\\frac{d}{dx}f(x) = \\lim_{\\delta \\to 0} \\frac{f(x+\\delta) - f(x-\\delta)}{2\\delta}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8zBdIyGJNk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gradient\n",
        "def Gradient(x: float or np.ndarray, t: float or np.ndarray)->tuple:\n",
        "    global W, b\n",
        "    pres_W = W.copy()\n",
        "    grad_W = np.zeros_like(W)\n",
        "    delta = 1e-7\n",
        "\n",
        "    for idx in range(W.size):     # x, 즉 입력하는 값들이 여러 개니까 w 또한 그 개수만큼!! 그래서 for문 돌리면서 기울기 구해주는 것! 각 변수에 대해 편미분!\n",
        "        W[idx,0] = pres_W[idx,0] + delta\n",
        "        cost_p = Cost(x, t)\n",
        "        W[idx,0] = pres_W[idx,0] - delta\n",
        "        cost_m = Cost(x, t)\n",
        "        grad_W[idx,0] = (cost_p-cost_m)/(2*delta)\n",
        "        W[idx,0] = pres_W[idx,0]\n",
        "\n",
        "    pres_b = b                   # 하지만 bias는 여전히 하나이기 때문에 이전 코드들과 동일!\n",
        "    b = pres_b + delta\n",
        "    cost_p = Cost(x, t)\n",
        "    b = pres_b - delta\n",
        "    cost_m = Cost(x, t)\n",
        "    grad_b = (cost_p-cost_m)/(2*delta)\n",
        "\n",
        "    return grad_W, grad_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyusRNVDJyaT",
        "colab_type": "text"
      },
      "source": [
        ">### 학습 준비 과정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY5btVVGG_Za",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe4ef0da-e41f-43e7-e76e-e77b1962e3d1"
      },
      "source": [
        "N_training = 1000000\n",
        "training_idx = np.arange(0, N_training+1, 1)\n",
        "cost_graph = np.zeros(N_training+1)\n",
        "\n",
        "cost_graph[0] = Cost(x_input, labels)\n",
        "print(\"[{:>5}] cost = {:>10.4}, W = [[{:>7.4}] [{:>7.4}] [{:>7.4}]], b = {:>7.4}\".format(0, cost_graph[0], W[0,0], W[1,0], W[2,0], b))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    0] cost =  2.862e+04, W = [[0.06111] [-0.6009] [ 0.6478]], b =  0.4474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rPELaJVKJfM",
        "colab_type": "text"
      },
      "source": [
        ">### Training\n",
        ">### $\\mu$ : Learning rate\n",
        ">### $W = W - \\mu\\frac{\\partial}{\\partial W}cost(W, b)$\n",
        ">### $b = b - \\mu\\frac{\\partial}{\\partial b}cost(W, b)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btm6fmLkKJza",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9c3b8cf5-6297-4572-b468-6b96328c38db"
      },
      "source": [
        "# 학습 (Training)\n",
        "learning_rate = 0.00003\n",
        "for cnt_training in range(1, N_training+1):\n",
        "    grad_W, grad_b = Gradient(x_input, labels)\n",
        "    cost_graph[cnt_training] = Cost(x_input, labels)\n",
        "    W -= learning_rate * grad_W\n",
        "    b -= learning_rate * grad_b\n",
        "    if cnt_training % 1000 == 0:\n",
        "        print(\"[{:>5}] cost = {:>10.4}, W = [[{:>7.4}] [{:>7.4}] [{:>7.4}]], b = {:>7.4}\".format(cnt_training, cost_graph[cnt_training], W[0,0], W[1,0], W[2,0], b))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1000] cost =      23.53, W = [[ 0.4599] [ 0.2638] [ 0.5788]], b =  0.4533\n",
            "[ 2000] cost =      23.37, W = [[ 0.4588] [ 0.2645] [ 0.5104]], b =  0.4531\n",
            "[ 3000] cost =      23.22, W = [[  0.456] [ 0.2658] [ 0.4423]], b =   0.453\n",
            "[ 4000] cost =      23.06, W = [[ 0.4531] [ 0.2671] [ 0.3744]], b =  0.4529\n",
            "[ 5000] cost =      22.91, W = [[ 0.4503] [ 0.2684] [ 0.3067]], b =  0.4528\n",
            "[ 6000] cost =      22.76, W = [[ 0.4475] [ 0.2697] [ 0.2393]], b =  0.4527\n",
            "[ 7000] cost =      22.61, W = [[ 0.4446] [ 0.2709] [ 0.1721]], b =  0.4525\n",
            "[ 8000] cost =      22.46, W = [[ 0.4418] [ 0.2722] [ 0.1051]], b =  0.4524\n",
            "[ 9000] cost =      22.31, W = [[  0.439] [ 0.2735] [0.03834]], b =  0.4523\n",
            "[10000] cost =      22.16, W = [[ 0.4362] [ 0.2748] [-0.0282]], b =  0.4522\n",
            "[11000] cost =      22.01, W = [[ 0.4334] [  0.276] [-0.09452]], b =  0.4521\n",
            "[12000] cost =      21.87, W = [[ 0.4306] [ 0.2773] [-0.1606]], b =  0.4519\n",
            "[13000] cost =      21.72, W = [[ 0.4279] [ 0.2785] [-0.2265]], b =  0.4518\n",
            "[14000] cost =      21.58, W = [[ 0.4251] [ 0.2798] [-0.2922]], b =  0.4517\n",
            "[15000] cost =      21.43, W = [[ 0.4224] [  0.281] [-0.3576]], b =  0.4516\n",
            "[16000] cost =      21.29, W = [[ 0.4196] [ 0.2823] [-0.4228]], b =  0.4515\n",
            "[17000] cost =      21.15, W = [[ 0.4169] [ 0.2835] [-0.4879]], b =  0.4513\n",
            "[18000] cost =      21.01, W = [[ 0.4142] [ 0.2847] [-0.5526]], b =  0.4512\n",
            "[19000] cost =      20.87, W = [[ 0.4114] [  0.286] [-0.6172]], b =  0.4511\n",
            "[20000] cost =      20.73, W = [[ 0.4087] [ 0.2872] [-0.6816]], b =   0.451\n",
            "[21000] cost =       20.6, W = [[  0.406] [ 0.2884] [-0.7457]], b =  0.4509\n",
            "[22000] cost =      20.46, W = [[ 0.4033] [ 0.2896] [-0.8097]], b =  0.4507\n",
            "[23000] cost =      20.32, W = [[ 0.4007] [ 0.2909] [-0.8734]], b =  0.4506\n",
            "[24000] cost =      20.19, W = [[  0.398] [ 0.2921] [-0.9369]], b =  0.4505\n",
            "[25000] cost =      20.06, W = [[ 0.3953] [ 0.2933] [   -1.0]], b =  0.4504\n",
            "[26000] cost =      19.92, W = [[ 0.3927] [ 0.2945] [ -1.063]], b =  0.4503\n",
            "[27000] cost =      19.79, W = [[   0.39] [ 0.2957] [ -1.126]], b =  0.4501\n",
            "[28000] cost =      19.66, W = [[ 0.3874] [ 0.2969] [ -1.189]], b =    0.45\n",
            "[29000] cost =      19.53, W = [[ 0.3848] [  0.298] [ -1.251]], b =  0.4499\n",
            "[30000] cost =       19.4, W = [[ 0.3822] [ 0.2992] [ -1.314]], b =  0.4498\n",
            "[31000] cost =      19.27, W = [[ 0.3795] [ 0.3004] [ -1.376]], b =  0.4497\n",
            "[32000] cost =      19.14, W = [[ 0.3769] [ 0.3016] [ -1.437]], b =  0.4495\n",
            "[33000] cost =      19.02, W = [[ 0.3744] [ 0.3028] [ -1.499]], b =  0.4494\n",
            "[34000] cost =      18.89, W = [[ 0.3718] [ 0.3039] [ -1.561]], b =  0.4493\n",
            "[35000] cost =      18.76, W = [[ 0.3692] [ 0.3051] [ -1.622]], b =  0.4492\n",
            "[36000] cost =      18.64, W = [[ 0.3666] [ 0.3063] [ -1.683]], b =  0.4491\n",
            "[37000] cost =      18.52, W = [[ 0.3641] [ 0.3074] [ -1.744]], b =  0.4489\n",
            "[38000] cost =      18.39, W = [[ 0.3615] [ 0.3086] [ -1.804]], b =  0.4488\n",
            "[39000] cost =      18.27, W = [[  0.359] [ 0.3097] [ -1.865]], b =  0.4487\n",
            "[40000] cost =      18.15, W = [[ 0.3565] [ 0.3109] [ -1.925]], b =  0.4486\n",
            "[41000] cost =      18.03, W = [[ 0.3539] [  0.312] [ -1.985]], b =  0.4485\n",
            "[42000] cost =      17.91, W = [[ 0.3514] [ 0.3132] [ -2.045]], b =  0.4484\n",
            "[43000] cost =      17.79, W = [[ 0.3489] [ 0.3143] [ -2.104]], b =  0.4482\n",
            "[44000] cost =      17.67, W = [[ 0.3464] [ 0.3154] [ -2.164]], b =  0.4481\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e5982eae9689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.00003\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcnt_training\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_training\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgrad_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcost_graph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnt_training\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mW\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-49d101fd2f76>\u001b[0m in \u001b[0;36mGradient\u001b[0;34m(x, t)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mcost_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpres_W\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mcost_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mgrad_W\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcost_p\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcost_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpres_W\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-1c3966441812>\u001b[0m in \u001b[0;36mCost\u001b[0;34m(x, t)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# return np.mean((Hypothesis(x) - t)**2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHypothesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3256\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3257\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW_z2m29K4Av",
        "colab_type": "text"
      },
      "source": [
        ">### Hypothesis Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_UwcVCzK4KD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "bec206bb-feca-4d52-97b4-c9a8e8585248"
      },
      "source": [
        "yes_or_no = [\"No\", \"Yes\"]\n",
        "\n",
        "print(\"[ Hypothesis test ]\")\n",
        "\n",
        "H_x = Hypothesis(x_input)\n",
        "for x,h,l in zip(x_input, H_x, labels):\n",
        "    print(\"Weight : {}, Height : {}, Smoking : {} => life-span : {} [label => {}]\".format(x[0],x[1],yes_or_no[int(x[2])],h,l[0]))\n",
        "\n",
        "x_test = [86, 183.4, 1.]\n",
        "H_x = Hypothesis(x_test)\n",
        "print(\"\\n[ Hypothesis test by another data ]\")\n",
        "print(\"Weight : {}, Height : {}, Smoking : {} => life-span : {}\".format(x_test[0],x_test[1],yes_or_no[int(x_test[2])],H_x))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ Hypothesis test ]\n",
            "Weight : 60.0, Height : 167.64, Smoking : Yes => life-span : [71.93599811] [label => 66]\n",
            "Weight : 65.0, Height : 152.4, Smoking : No => life-span : [71.03935923] [label => 74]\n",
            "Weight : 55.0, Height : 182.88, Smoking : Yes => life-span : [75.02551437] [label => 78]\n",
            "\n",
            "[ Hypothesis test by another data ]\n",
            "Weight : 86, Height : 183.4, Smoking : Yes => life-span : [85.89051863]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0pdyAxHKZDE",
        "colab_type": "text"
      },
      "source": [
        ">### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4ZTUgp9J3Tn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "509ab9eb-f042-4d9b-92eb-e414f8822ab8"
      },
      "source": [
        "# Training 상황에 대한 그래프 출력\n",
        "fig_cost, ax_cost = plt.subplots()\n",
        "ax_cost.plot(training_idx, cost_graph)\n",
        "ax_cost.set_title(\"'Cost / Training Count' Graph\")\n",
        "ax_cost.set_xlim(0, 400)\n",
        "ax_cost.set_xlabel(\"Train Count\")\n",
        "ax_cost.set_ylabel(\"Cost\")\n",
        "ax_cost.grid(True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFLflOAkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTLmjIDKZSja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}