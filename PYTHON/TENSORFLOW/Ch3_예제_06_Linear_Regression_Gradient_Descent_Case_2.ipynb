{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch3_예제_06_Linear_Regression_Gradient_Descent_Case_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KwonYN/TIL/blob/master/PYTHON/TENSORFLOW/Ch3_%EC%98%88%EC%A0%9C_06_Linear_Regression_Gradient_Descent_Case_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBMFIOU_oWOg",
        "colab_type": "text"
      },
      "source": [
        "# Chapter3-2. Deep Learning 기초 : Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w690xaWJoaQU",
        "colab_type": "text"
      },
      "source": [
        ">## [예제3-6] Gradient Descent of Linear Regression : Case #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuxRw2vAo5Fi",
        "colab_type": "text"
      },
      "source": [
        ">### Load modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNPBqbLgGfUs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d9c97fbb-98f5-42b4-d28e-e3af72596a76"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Module Loaded.\")\n",
        "print(\"NumPy Version :{}\".format(np.__version__))\n",
        "print(\"Matplotlib Version :{}\".format(plt.matplotlib.__version__))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Module Loaded.\n",
            "NumPy Version :1.17.4\n",
            "Matplotlib Version :3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OctX9GUDJ6Z3",
        "colab_type": "text"
      },
      "source": [
        "> ### Input and Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNt07NUKKx6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input and Labels\n",
        "x_input = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype= np.float)\n",
        "labels = np.array([3, 5, 7, 9, 11, 13, 15, 17, 19, 21], dtype= np.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaUmHQgGA8-d",
        "colab_type": "text"
      },
      "source": [
        ">### Hypothesis : Linear Equation\n",
        ">### $h(x) = wx + b$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td6Qr3VuNFug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Weight and Bias\n",
        "w = np.random.normal()\n",
        "b = np.random.normal()\n",
        "\n",
        "# Hypothesis : Linear Function\n",
        "def Hypothesis(x: float or np.ndarray)->float or np.ndarray:    \n",
        "    return w*x + b\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BFpxFy_0pMRz"
      },
      "source": [
        ">### Cost Function : Mean Squared Error (MSE)\n",
        ">### $\\sum_{i=1}^{n}(h(x_{i})-y_{i})^{2}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szvt5sniBQiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cost : Mean Squared Error \n",
        "def Cost(x: float or np.ndarray, t: float or np.ndarray)->float or np.ndarray:\n",
        "    return np.mean((Hypothesis(x) - t)**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxyl6ifpBJfZ",
        "colab_type": "text"
      },
      "source": [
        ">### Gradient : 미분의 다른 정의 이용\n",
        ">### $\\frac{d}{dx}f(x) = \\lim_{\\delta = 0} \\frac{f(x+\\delta) - f(x-\\delta)}{2\\delta}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urqcydR4shiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gradient \n",
        "def Gradient(x: float or np.ndarray, t: float or np.ndarray)->tuple:\n",
        "    global w, b\n",
        "    pres_w = w\n",
        "    delta = 1e-7\n",
        "\n",
        "    w = pres_w + delta\n",
        "    cost_p = Cost(x, t)\n",
        "    w = pres_w - delta\n",
        "    cost_m = Cost(x, t)\n",
        "    grad_w = (cost_p-cost_m)/(2*delta)\n",
        "    w = pres_w\n",
        "\n",
        "    pres_b = b\n",
        "    b = pres_b + delta\n",
        "    cost_p = Cost(x, t)\n",
        "    b = pres_b - delta\n",
        "    cost_m = Cost(x, t)\n",
        "    grad_b = (cost_p-cost_m)/(2*delta)\n",
        "\n",
        "    return grad_w, grad_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TQtA-0TwFWy",
        "colab_type": "text"
      },
      "source": [
        ">### 학습 준비 과정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyU_Yw3FuxkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_training = 3000\n",
        "learning_rate = 0.01\n",
        "\n",
        "training_idx = np.arange(0, N_training+1, 1)\n",
        "cost_graph = np.zeros(N_training+1)\n",
        "\n",
        "cost_graph[0] = Cost(x_input, labels)\n",
        "print(\"[{:>5}] cost = {:>10.4}, w = {:>7.4}, b={:>7.4}\".format(0, cost_graph[0], float(w), float(b)))\n",
        "\n",
        "check = np.array([0, 1, 2, 3, 4, N_training])\n",
        "w_trained = np.zeros(check.size)\n",
        "b_trained = np.zeros(check.size)\n",
        "w_trained[0] = w\n",
        "b_trained[0] = b\n",
        "check_idx = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qwbFA4-wQFE",
        "colab_type": "text"
      },
      "source": [
        ">### Training\n",
        ">### $\\mu$ : Learning rate\n",
        ">### $w = w - \\mu\\frac{\\partial}{\\partial w}cost(w, b)$\n",
        ">### $b = b - \\mu\\frac{\\partial}{\\partial b}cost(w, b)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzaB4hcauxx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습 (Training)\n",
        "for cnt_training in range(1, N_training+1):\n",
        "    grad_w, grad_b = Gradient(x_input, labels)\n",
        "    w -= learning_rate * grad_w\n",
        "    b -= learning_rate * grad_b\n",
        "    cost_graph[cnt_training] = Cost(x_input, labels)\n",
        "    if check[check_idx] == cnt_training:\n",
        "        w_trained[check_idx] = w\n",
        "        b_trained[check_idx] = b\n",
        "        check_idx += 1\n",
        "    if cnt_training % 100 == 0:\n",
        "        print(\"[{:>5}] cost = {:>10.4}, w = {:>7.4}, b = {:>7.4}\".format(cnt_training, cost_graph[cnt_training], w, b))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQvZqYzAxU_M",
        "colab_type": "text"
      },
      "source": [
        ">### Plotting Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52vI_ZGeu2Ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training 상황에 대한 그래프 출력\n",
        "# 1)Training 회수 별 Cost 값\n",
        "fig_cost, ax_cost = plt.subplots()\n",
        "ax_cost.plot(training_idx, cost_graph)\n",
        "ax_cost.set_title(\"'Cost / Training Count' Graph\")\n",
        "ax_cost.set_xlim(0, 20)\n",
        "ax_cost.set_xlabel(\"Train Count\")\n",
        "ax_cost.set_ylabel(\"Cost\")\n",
        "ax_cost.grid(True)\n",
        "\n",
        "# 2)Training 회수에 따른 Hypothesis의 상황\n",
        "x_hypo = np.linspace(0, 11, 1000)\n",
        "fig_hypothesis, ax_hypothesis = plt.subplots(2, 3, figsize=(15,10))\n",
        "fig_hypothesis.suptitle(\"'Hypothesis / Training Count' Graph\")\n",
        "for ax_idx in range(check.size):\n",
        "    w = w_trained[ax_idx]\n",
        "    b = b_trained[ax_idx]\n",
        "    y_hypo = Hypothesis(x_hypo)\n",
        "    ax_hypothesis[ax_idx // 3][ax_idx % 3].plot(x_hypo, y_hypo, label='hypothesis')\n",
        "    ax_hypothesis[ax_idx // 3][ax_idx % 3].scatter(x_input, labels, color='orange', label='labels')\n",
        "    ax_hypothesis[ax_idx // 3][ax_idx % 3].set_title(\"Train Count : {}\".format(check[ax_idx]))\n",
        "    ax_hypothesis[ax_idx // 3][ax_idx % 3].set_ylim((-25, 25))\n",
        "    ax_hypothesis[ax_idx // 3][ax_idx % 3].set_xlabel(\"x\")\n",
        "    ax_hypothesis[ax_idx // 3][ax_idx % 3].set_ylabel(\"y\")\n",
        "    ax_hypothesis[ax_idx // 3][ax_idx % 3].legend()\n",
        "    ax_hypothesis[ax_idx // 3][ax_idx % 3].grid(True)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaNBuJWfvzCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}