{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_02_char_rnn_7_kyn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KwonYN/TIL/blob/master/PYTHON/TENSORFLOW2/03_02_char_rnn_7_kyn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUbrsmxWf04y",
        "colab_type": "text"
      },
      "source": [
        "# **실습 3-2 : Char-RNN**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZQKizrb-Y1er"
      },
      "source": [
        "## **Import Module**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0trJmd6DjqBZ",
        "outputId": "92dfb358-bd15-4913-cf1f-bb4dbcf457a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOiaPQ3xgJQC",
        "colab_type": "text"
      },
      "source": [
        "## **DataSet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD28IHuegUxp",
        "colab_type": "text"
      },
      "source": [
        "### 학습할 문장 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC8AS287Hz-9",
        "colab_type": "code",
        "outputId": "03601774-7911-474c-e964-7a5a313a52c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "# 학습할 문장\n",
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")\n",
        "\n",
        "print (\"FOLLOWING IS OUR TRAINING SEQUENCE:\")\n",
        "print (sentence)\n",
        "print (\"Length of 'test sentence' is %s\" %len(sentence))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLLOWING IS OUR TRAINING SEQUENCE:\n",
            "if you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "Length of 'test sentence' is 180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0oDy0CggOEm",
        "colab_type": "text"
      },
      "source": [
        "### 입력 문자열과 타겟 문자 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n1JO20hopHt-"
      },
      "source": [
        "#### Char dictionary 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7d2RtBMDI6a",
        "colab_type": "code",
        "outputId": "eef87e71-a671-4299-c4d6-5f082d0a1ba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "# make charater dictionary \n",
        "char_set = list(set(sentence))\n",
        "char_dic = {w: i for i, w in enumerate(char_set)}\n",
        "print (\"CHARACTERS: \")\n",
        "print (len(char_set))\n",
        "print (char_set)\n",
        "print (\"DICTIONARY: \")\n",
        "print (len(char_dic))\n",
        "print (char_dic)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CHARACTERS: \n",
            "25\n",
            "['t', 'd', 'f', 'u', \"'\", 'c', 'b', 'p', ',', ' ', 'r', 'e', 'i', 'k', 'h', 'w', 'y', 'g', 's', 'a', 'l', 'o', 'n', 'm', '.']\n",
            "DICTIONARY: \n",
            "25\n",
            "{'t': 0, 'd': 1, 'f': 2, 'u': 3, \"'\": 4, 'c': 5, 'b': 6, 'p': 7, ',': 8, ' ': 9, 'r': 10, 'e': 11, 'i': 12, 'k': 13, 'h': 14, 'w': 15, 'y': 16, 'g': 17, 's': 18, 'a': 19, 'l': 20, 'o': 21, 'n': 22, 'm': 23, '.': 24}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rXpRna2MpGvz"
      },
      "source": [
        "#### 문자열"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LXjO9txq9nR",
        "colab_type": "code",
        "outputId": "f3e1850c-6c45-49f9-f608-0c21a0780af8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "data_dim    = len(char_set) # train data X:input\n",
        "num_classes = len(char_set) # trian data Y:target\n",
        "sequence_length = 10        # any arbitrary number\n",
        "print ('data_dim : %d' %data_dim)\n",
        "print ('num_classes : %d' %num_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_dim : 25\n",
            "num_classes : 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGnO0NygDI6M",
        "colab_type": "code",
        "outputId": "3a9657e5-87c1-4241-cf3b-f034d5d49282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "dataX = []  # input sequence list\n",
        "dataY = []  # target sequence list \n",
        "\n",
        "# we will make 170 sequences \n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "    # 10 characters = 1 training sequence\n",
        "    x_str = sentence[i : i+sequence_length]\n",
        "    y_str = sentence[i+1 : i+sequence_length+1]\n",
        "    # convert x, y str to index(int)\n",
        "    x_idx = [char_dic[c] for c in x_str] \n",
        "    y_idx = [char_dic[c] for c in y_str] \n",
        "    # append to dataset list\n",
        "    dataX.append(x_idx)\n",
        "    dataY.append(y_idx)\n",
        "\n",
        "    # monitoring\n",
        "    if i<5:\n",
        "        print (\"[%3d/%3d] [%s]=>[%s]\" % (i, len(sentence), x_str, y_str))\n",
        "        print (\"%s%s=>%s\" % (' '*10, x_idx, y_idx))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0/180] [if you wan]=>[f you want]\n",
            "          [12, 2, 9, 16, 21, 3, 9, 15, 19, 22]=>[2, 9, 16, 21, 3, 9, 15, 19, 22, 0]\n",
            "[  1/180] [f you want]=>[ you want ]\n",
            "          [2, 9, 16, 21, 3, 9, 15, 19, 22, 0]=>[9, 16, 21, 3, 9, 15, 19, 22, 0, 9]\n",
            "[  2/180] [ you want ]=>[you want t]\n",
            "          [9, 16, 21, 3, 9, 15, 19, 22, 0, 9]=>[16, 21, 3, 9, 15, 19, 22, 0, 9, 0]\n",
            "[  3/180] [you want t]=>[ou want to]\n",
            "          [16, 21, 3, 9, 15, 19, 22, 0, 9, 0]=>[21, 3, 9, 15, 19, 22, 0, 9, 0, 21]\n",
            "[  4/180] [ou want to]=>[u want to ]\n",
            "          [21, 3, 9, 15, 19, 22, 0, 9, 0, 21]=>[3, 9, 15, 19, 22, 0, 9, 0, 21, 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA7IGA2nZwhf",
        "colab_type": "code",
        "outputId": "867faedd-bc74-44f0-d38a-3d0ed7574498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# check type and data\n",
        "print('\\n')\n",
        "print ((type(dataX)))\n",
        "print (dataX[0])\n",
        "print (dataX[1])\n",
        "print (dataX[168])\n",
        "print (dataX[169])\n",
        "\n",
        "print('\\n')\n",
        "print ((type(dataY)))\n",
        "print (dataY[0])\n",
        "print (dataY[1])\n",
        "print (dataY[168])\n",
        "print (dataY[169])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "<class 'list'>\n",
            "[12, 2, 9, 16, 21, 3, 9, 15, 19, 22]\n",
            "[2, 9, 16, 21, 3, 9, 15, 19, 22, 0]\n",
            "[9, 21, 2, 9, 0, 14, 11, 9, 18, 11]\n",
            "[21, 2, 9, 0, 14, 11, 9, 18, 11, 19]\n",
            "\n",
            "\n",
            "<class 'list'>\n",
            "[2, 9, 16, 21, 3, 9, 15, 19, 22, 0]\n",
            "[9, 16, 21, 3, 9, 15, 19, 22, 0, 9]\n",
            "[21, 2, 9, 0, 14, 11, 9, 18, 11, 19]\n",
            "[2, 9, 0, 14, 11, 9, 18, 11, 19, 24]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59y_7OfhvMpY",
        "colab_type": "text"
      },
      "source": [
        "### 입력문자 list를 3-dim로, 타겟문자 list를 2-dim로 변환  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obs8_hpYv1gd",
        "colab_type": "code",
        "outputId": "27631943-ea62-487f-d78d-09999575e100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "# input tensor 생성 \n",
        "X = np.array(dataX)                                      # ndarray(170,10)<-[170,10]\n",
        "# convert list sequence to 3dim array(one-hot coding)\n",
        "sequences = [tf.keras.utils.to_categorical(\n",
        "                 x, num_classes = data_dim) for x in X]  # list[170,10,25]\n",
        "X = np.array(sequences)                                  # ndarray(170,10,25)\n",
        "print(\"shape of X :\", X.shape)\n",
        "\n",
        "# Target tensor 생성\n",
        "y = np.array(dataY)[:,-1]                                # ndarray(170,)<-[170,10]\n",
        "print(\"shape of y :\", y.shape)\n",
        "\n",
        "# convert the vector to 2-dim \n",
        "y = tf.keras.utils.to_categorical(\n",
        "                  y, num_classes = data_dim)             # (170,25)\n",
        "\n",
        "print (\"shape of y :\", y.shape) # (170, 25)\n",
        "print (y[0])    # t -> one-hot ,  (if you wan't')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of X : (170, 10, 25)\n",
            "shape of y : (170,)\n",
            "shape of y : (170, 25)\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY3zfzjlgbYj",
        "colab_type": "text"
      },
      "source": [
        "## **Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUqUyRtngdUC",
        "colab_type": "text"
      },
      "source": [
        "### Define"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCAvqXiiDI6X",
        "colab_type": "code",
        "outputId": "b3fae102-97a8-4c94-e6b9-31e388d86afb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Sequenctial model define: LSTM + Dense\n",
        "hidden_dim = 75\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.LSTM(units=hidden_dim,\n",
        "                               input_shape=(sequence_length,num_classes)))\n",
        "model.add(tf.keras.layers.Dense(data_dim, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 75)                30300     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 25)                1900      \n",
            "=================================================================\n",
            "Total params: 32,200\n",
            "Trainable params: 32,200\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zMat0-hSzeq_"
      },
      "source": [
        "### Compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3VyAq7gzkuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', # one-hot coding\n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9BG8hNUgo0n",
        "colab_type": "text"
      },
      "source": [
        "### Fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KbIreYLd6gj",
        "colab_type": "text"
      },
      "source": [
        "Epoch 1000/1000   \n",
        "170/170 [==============================] - 0s 235us/sample - loss: 0.0012 - accuracy: 1.0000    \n",
        "CPU times: user 48.3 s, sys: 6.54 s, total: 54.8 s    \n",
        "Wall time: 47.8 s (@Notebook Setting/GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_idMKYwKVqX",
        "colab_type": "code",
        "outputId": "29220d3a-51ac-4b21-886f-ab7a7a9309c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "model.fit(X,            # X.shape : (170, 10, 25)\n",
        "          y,            # y.shape : (170, 25)\n",
        "          epochs=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 170 samples\n",
            "Epoch 1/200\n",
            "170/170 [==============================] - 3s 16ms/sample - loss: 3.2127 - accuracy: 0.1059\n",
            "Epoch 2/200\n",
            "170/170 [==============================] - 0s 174us/sample - loss: 3.1773 - accuracy: 0.1941\n",
            "Epoch 3/200\n",
            "170/170 [==============================] - 0s 189us/sample - loss: 3.1407 - accuracy: 0.1882\n",
            "Epoch 4/200\n",
            "170/170 [==============================] - 0s 174us/sample - loss: 3.0859 - accuracy: 0.1882\n",
            "Epoch 5/200\n",
            "170/170 [==============================] - 0s 150us/sample - loss: 3.0002 - accuracy: 0.1882\n",
            "Epoch 6/200\n",
            "170/170 [==============================] - 0s 156us/sample - loss: 2.9003 - accuracy: 0.1882\n",
            "Epoch 7/200\n",
            "170/170 [==============================] - 0s 163us/sample - loss: 2.8553 - accuracy: 0.1882\n",
            "Epoch 8/200\n",
            "170/170 [==============================] - 0s 171us/sample - loss: 2.8229 - accuracy: 0.1882\n",
            "Epoch 9/200\n",
            "170/170 [==============================] - 0s 221us/sample - loss: 2.8046 - accuracy: 0.1882\n",
            "Epoch 10/200\n",
            "170/170 [==============================] - 0s 152us/sample - loss: 2.7911 - accuracy: 0.1882\n",
            "Epoch 11/200\n",
            "170/170 [==============================] - 0s 157us/sample - loss: 2.7775 - accuracy: 0.1882\n",
            "Epoch 12/200\n",
            "170/170 [==============================] - 0s 175us/sample - loss: 2.7621 - accuracy: 0.1882\n",
            "Epoch 13/200\n",
            "170/170 [==============================] - 0s 178us/sample - loss: 2.7609 - accuracy: 0.1882\n",
            "Epoch 14/200\n",
            "170/170 [==============================] - 0s 155us/sample - loss: 2.7470 - accuracy: 0.1882\n",
            "Epoch 15/200\n",
            "170/170 [==============================] - 0s 148us/sample - loss: 2.7251 - accuracy: 0.1882\n",
            "Epoch 16/200\n",
            "170/170 [==============================] - 0s 150us/sample - loss: 2.7055 - accuracy: 0.1882\n",
            "Epoch 17/200\n",
            "170/170 [==============================] - 0s 195us/sample - loss: 2.6918 - accuracy: 0.1882\n",
            "Epoch 18/200\n",
            "170/170 [==============================] - 0s 184us/sample - loss: 2.6734 - accuracy: 0.1882\n",
            "Epoch 19/200\n",
            "170/170 [==============================] - 0s 189us/sample - loss: 2.6455 - accuracy: 0.1882\n",
            "Epoch 20/200\n",
            "170/170 [==============================] - 0s 163us/sample - loss: 2.6220 - accuracy: 0.1882\n",
            "Epoch 21/200\n",
            "170/170 [==============================] - 0s 156us/sample - loss: 2.5945 - accuracy: 0.1941\n",
            "Epoch 22/200\n",
            "170/170 [==============================] - 0s 155us/sample - loss: 2.5655 - accuracy: 0.2059\n",
            "Epoch 23/200\n",
            "170/170 [==============================] - 0s 156us/sample - loss: 2.5227 - accuracy: 0.2059\n",
            "Epoch 24/200\n",
            "170/170 [==============================] - 0s 190us/sample - loss: 2.5004 - accuracy: 0.2294\n",
            "Epoch 25/200\n",
            "170/170 [==============================] - 0s 193us/sample - loss: 2.4682 - accuracy: 0.2118\n",
            "Epoch 26/200\n",
            "170/170 [==============================] - 0s 158us/sample - loss: 2.4441 - accuracy: 0.2294\n",
            "Epoch 27/200\n",
            "170/170 [==============================] - 0s 151us/sample - loss: 2.3986 - accuracy: 0.2412\n",
            "Epoch 28/200\n",
            "170/170 [==============================] - 0s 164us/sample - loss: 2.3751 - accuracy: 0.2294\n",
            "Epoch 29/200\n",
            "170/170 [==============================] - 0s 171us/sample - loss: 2.3478 - accuracy: 0.2706\n",
            "Epoch 30/200\n",
            "170/170 [==============================] - 0s 154us/sample - loss: 2.3024 - accuracy: 0.2824\n",
            "Epoch 31/200\n",
            "170/170 [==============================] - 0s 197us/sample - loss: 2.2798 - accuracy: 0.2941\n",
            "Epoch 32/200\n",
            "170/170 [==============================] - 0s 148us/sample - loss: 2.2429 - accuracy: 0.3294\n",
            "Epoch 33/200\n",
            "170/170 [==============================] - 0s 159us/sample - loss: 2.2207 - accuracy: 0.3294\n",
            "Epoch 34/200\n",
            "170/170 [==============================] - 0s 161us/sample - loss: 2.1873 - accuracy: 0.3000\n",
            "Epoch 35/200\n",
            "170/170 [==============================] - 0s 160us/sample - loss: 2.1676 - accuracy: 0.2765\n",
            "Epoch 36/200\n",
            "170/170 [==============================] - 0s 156us/sample - loss: 2.1317 - accuracy: 0.3059\n",
            "Epoch 37/200\n",
            "170/170 [==============================] - 0s 170us/sample - loss: 2.1034 - accuracy: 0.3471\n",
            "Epoch 38/200\n",
            "170/170 [==============================] - 0s 154us/sample - loss: 2.0750 - accuracy: 0.4059\n",
            "Epoch 39/200\n",
            "170/170 [==============================] - 0s 180us/sample - loss: 2.0468 - accuracy: 0.3706\n",
            "Epoch 40/200\n",
            "170/170 [==============================] - 0s 187us/sample - loss: 2.0054 - accuracy: 0.3941\n",
            "Epoch 41/200\n",
            "170/170 [==============================] - 0s 167us/sample - loss: 1.9713 - accuracy: 0.4412\n",
            "Epoch 42/200\n",
            "170/170 [==============================] - 0s 161us/sample - loss: 1.9570 - accuracy: 0.4176\n",
            "Epoch 43/200\n",
            "170/170 [==============================] - 0s 164us/sample - loss: 1.9223 - accuracy: 0.4059\n",
            "Epoch 44/200\n",
            "170/170 [==============================] - 0s 153us/sample - loss: 1.8885 - accuracy: 0.4588\n",
            "Epoch 45/200\n",
            "170/170 [==============================] - 0s 157us/sample - loss: 1.8683 - accuracy: 0.4824\n",
            "Epoch 46/200\n",
            "170/170 [==============================] - 0s 159us/sample - loss: 1.8307 - accuracy: 0.5059\n",
            "Epoch 47/200\n",
            "170/170 [==============================] - 0s 182us/sample - loss: 1.8351 - accuracy: 0.4412\n",
            "Epoch 48/200\n",
            "170/170 [==============================] - 0s 202us/sample - loss: 1.7679 - accuracy: 0.4882\n",
            "Epoch 49/200\n",
            "170/170 [==============================] - 0s 165us/sample - loss: 1.7490 - accuracy: 0.5059\n",
            "Epoch 50/200\n",
            "170/170 [==============================] - 0s 154us/sample - loss: 1.6957 - accuracy: 0.5059\n",
            "Epoch 51/200\n",
            "170/170 [==============================] - 0s 148us/sample - loss: 1.6639 - accuracy: 0.5353\n",
            "Epoch 52/200\n",
            "170/170 [==============================] - 0s 156us/sample - loss: 1.6686 - accuracy: 0.5176\n",
            "Epoch 53/200\n",
            "170/170 [==============================] - 0s 180us/sample - loss: 1.6683 - accuracy: 0.4941\n",
            "Epoch 54/200\n",
            "170/170 [==============================] - 0s 148us/sample - loss: 1.5848 - accuracy: 0.5529\n",
            "Epoch 55/200\n",
            "170/170 [==============================] - 0s 169us/sample - loss: 1.5598 - accuracy: 0.5706\n",
            "Epoch 56/200\n",
            "170/170 [==============================] - 0s 175us/sample - loss: 1.5097 - accuracy: 0.5588\n",
            "Epoch 57/200\n",
            "170/170 [==============================] - 0s 174us/sample - loss: 1.4921 - accuracy: 0.5471\n",
            "Epoch 58/200\n",
            "170/170 [==============================] - 0s 161us/sample - loss: 1.4507 - accuracy: 0.6176\n",
            "Epoch 59/200\n",
            "170/170 [==============================] - 0s 160us/sample - loss: 1.4132 - accuracy: 0.6353\n",
            "Epoch 60/200\n",
            "170/170 [==============================] - 0s 192us/sample - loss: 1.3863 - accuracy: 0.6176\n",
            "Epoch 61/200\n",
            "170/170 [==============================] - 0s 171us/sample - loss: 1.3698 - accuracy: 0.6412\n",
            "Epoch 62/200\n",
            "170/170 [==============================] - 0s 180us/sample - loss: 1.3318 - accuracy: 0.6235\n",
            "Epoch 63/200\n",
            "170/170 [==============================] - 0s 172us/sample - loss: 1.3126 - accuracy: 0.6529\n",
            "Epoch 64/200\n",
            "170/170 [==============================] - 0s 152us/sample - loss: 1.2718 - accuracy: 0.6765\n",
            "Epoch 65/200\n",
            "170/170 [==============================] - 0s 180us/sample - loss: 1.2562 - accuracy: 0.7000\n",
            "Epoch 66/200\n",
            "170/170 [==============================] - 0s 198us/sample - loss: 1.2423 - accuracy: 0.6529\n",
            "Epoch 67/200\n",
            "170/170 [==============================] - 0s 169us/sample - loss: 1.1813 - accuracy: 0.7118\n",
            "Epoch 68/200\n",
            "170/170 [==============================] - 0s 155us/sample - loss: 1.1911 - accuracy: 0.6882\n",
            "Epoch 69/200\n",
            "170/170 [==============================] - 0s 162us/sample - loss: 1.1432 - accuracy: 0.7235\n",
            "Epoch 70/200\n",
            "170/170 [==============================] - 0s 164us/sample - loss: 1.1219 - accuracy: 0.6941\n",
            "Epoch 71/200\n",
            "170/170 [==============================] - 0s 158us/sample - loss: 1.0839 - accuracy: 0.7471\n",
            "Epoch 72/200\n",
            "170/170 [==============================] - 0s 152us/sample - loss: 1.0644 - accuracy: 0.7529\n",
            "Epoch 73/200\n",
            "170/170 [==============================] - 0s 155us/sample - loss: 1.0285 - accuracy: 0.7471\n",
            "Epoch 74/200\n",
            "170/170 [==============================] - 0s 170us/sample - loss: 1.0123 - accuracy: 0.7824\n",
            "Epoch 75/200\n",
            "170/170 [==============================] - 0s 162us/sample - loss: 0.9772 - accuracy: 0.7647\n",
            "Epoch 76/200\n",
            "170/170 [==============================] - 0s 187us/sample - loss: 0.9574 - accuracy: 0.7824\n",
            "Epoch 77/200\n",
            "170/170 [==============================] - 0s 169us/sample - loss: 0.9328 - accuracy: 0.7882\n",
            "Epoch 78/200\n",
            "170/170 [==============================] - 0s 150us/sample - loss: 0.8917 - accuracy: 0.7824\n",
            "Epoch 79/200\n",
            "170/170 [==============================] - 0s 150us/sample - loss: 0.8743 - accuracy: 0.8000\n",
            "Epoch 80/200\n",
            "170/170 [==============================] - 0s 144us/sample - loss: 0.8506 - accuracy: 0.8059\n",
            "Epoch 81/200\n",
            "170/170 [==============================] - 0s 166us/sample - loss: 0.8164 - accuracy: 0.8235\n",
            "Epoch 82/200\n",
            "170/170 [==============================] - 0s 189us/sample - loss: 0.7923 - accuracy: 0.8353\n",
            "Epoch 83/200\n",
            "170/170 [==============================] - 0s 174us/sample - loss: 0.7759 - accuracy: 0.8118\n",
            "Epoch 84/200\n",
            "170/170 [==============================] - 0s 155us/sample - loss: 0.7579 - accuracy: 0.8588\n",
            "Epoch 85/200\n",
            "170/170 [==============================] - 0s 159us/sample - loss: 0.7531 - accuracy: 0.8353\n",
            "Epoch 86/200\n",
            "170/170 [==============================] - 0s 154us/sample - loss: 0.7158 - accuracy: 0.8706\n",
            "Epoch 87/200\n",
            "170/170 [==============================] - 0s 152us/sample - loss: 0.7156 - accuracy: 0.8765\n",
            "Epoch 88/200\n",
            "170/170 [==============================] - 0s 166us/sample - loss: 0.7020 - accuracy: 0.8882\n",
            "Epoch 89/200\n",
            "170/170 [==============================] - 0s 169us/sample - loss: 0.6687 - accuracy: 0.8706\n",
            "Epoch 90/200\n",
            "170/170 [==============================] - 0s 157us/sample - loss: 0.6488 - accuracy: 0.8941\n",
            "Epoch 91/200\n",
            "170/170 [==============================] - 0s 164us/sample - loss: 0.6158 - accuracy: 0.9294\n",
            "Epoch 92/200\n",
            "170/170 [==============================] - 0s 169us/sample - loss: 0.5939 - accuracy: 0.9294\n",
            "Epoch 93/200\n",
            "170/170 [==============================] - 0s 162us/sample - loss: 0.5679 - accuracy: 0.9353\n",
            "Epoch 94/200\n",
            "170/170 [==============================] - 0s 172us/sample - loss: 0.5477 - accuracy: 0.9412\n",
            "Epoch 95/200\n",
            "170/170 [==============================] - 0s 202us/sample - loss: 0.5331 - accuracy: 0.9412\n",
            "Epoch 96/200\n",
            "170/170 [==============================] - 0s 165us/sample - loss: 0.5075 - accuracy: 0.9765\n",
            "Epoch 97/200\n",
            "170/170 [==============================] - 0s 187us/sample - loss: 0.4866 - accuracy: 0.9765\n",
            "Epoch 98/200\n",
            "170/170 [==============================] - 0s 164us/sample - loss: 0.4698 - accuracy: 0.9882\n",
            "Epoch 99/200\n",
            "170/170 [==============================] - 0s 186us/sample - loss: 0.4566 - accuracy: 0.9765\n",
            "Epoch 100/200\n",
            "170/170 [==============================] - 0s 221us/sample - loss: 0.4327 - accuracy: 0.9765\n",
            "Epoch 101/200\n",
            "170/170 [==============================] - 0s 169us/sample - loss: 0.4181 - accuracy: 0.9824\n",
            "Epoch 102/200\n",
            "170/170 [==============================] - 0s 168us/sample - loss: 0.4048 - accuracy: 0.9824\n",
            "Epoch 103/200\n",
            "170/170 [==============================] - 0s 163us/sample - loss: 0.3843 - accuracy: 0.9882\n",
            "Epoch 104/200\n",
            "170/170 [==============================] - 0s 177us/sample - loss: 0.3741 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "170/170 [==============================] - 0s 182us/sample - loss: 0.3593 - accuracy: 0.9882\n",
            "Epoch 106/200\n",
            "170/170 [==============================] - 0s 175us/sample - loss: 0.3494 - accuracy: 0.9941\n",
            "Epoch 107/200\n",
            "170/170 [==============================] - 0s 152us/sample - loss: 0.3399 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "170/170 [==============================] - 0s 155us/sample - loss: 0.3304 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "170/170 [==============================] - 0s 152us/sample - loss: 0.3186 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "170/170 [==============================] - 0s 161us/sample - loss: 0.3049 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "170/170 [==============================] - 0s 153us/sample - loss: 0.2941 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "170/170 [==============================] - 0s 158us/sample - loss: 0.2806 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "170/170 [==============================] - 0s 165us/sample - loss: 0.2733 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "170/170 [==============================] - 0s 167us/sample - loss: 0.2665 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "170/170 [==============================] - 0s 181us/sample - loss: 0.2611 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "170/170 [==============================] - 0s 148us/sample - loss: 0.2478 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "170/170 [==============================] - 0s 158us/sample - loss: 0.2441 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "170/170 [==============================] - 0s 157us/sample - loss: 0.2325 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "170/170 [==============================] - 0s 174us/sample - loss: 0.2243 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "170/170 [==============================] - 0s 229us/sample - loss: 0.2215 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "170/170 [==============================] - 0s 203us/sample - loss: 0.2075 - accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "170/170 [==============================] - 0s 173us/sample - loss: 0.2030 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "170/170 [==============================] - 0s 166us/sample - loss: 0.1993 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "170/170 [==============================] - 0s 176us/sample - loss: 0.1897 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "170/170 [==============================] - 0s 182us/sample - loss: 0.1846 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "170/170 [==============================] - 0s 162us/sample - loss: 0.1800 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "170/170 [==============================] - 0s 159us/sample - loss: 0.1755 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "170/170 [==============================] - 0s 151us/sample - loss: 0.1682 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "170/170 [==============================] - 0s 149us/sample - loss: 0.1684 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "170/170 [==============================] - 0s 152us/sample - loss: 0.1616 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "170/170 [==============================] - 0s 146us/sample - loss: 0.1576 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "170/170 [==============================] - 0s 200us/sample - loss: 0.1512 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "170/170 [==============================] - 0s 189us/sample - loss: 0.1530 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "170/170 [==============================] - 0s 217us/sample - loss: 0.1448 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "170/170 [==============================] - 0s 157us/sample - loss: 0.1423 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "170/170 [==============================] - 0s 151us/sample - loss: 0.1367 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "170/170 [==============================] - 0s 160us/sample - loss: 0.1337 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "170/170 [==============================] - 0s 158us/sample - loss: 0.1295 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "170/170 [==============================] - 0s 168us/sample - loss: 0.1256 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "170/170 [==============================] - 0s 177us/sample - loss: 0.1244 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "170/170 [==============================] - 0s 149us/sample - loss: 0.1209 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "170/170 [==============================] - 0s 173us/sample - loss: 0.1171 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "170/170 [==============================] - 0s 201us/sample - loss: 0.1135 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "170/170 [==============================] - 0s 166us/sample - loss: 0.1124 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "170/170 [==============================] - 0s 198us/sample - loss: 0.1078 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "170/170 [==============================] - 0s 201us/sample - loss: 0.1055 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "170/170 [==============================] - 0s 177us/sample - loss: 0.1041 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "170/170 [==============================] - 0s 166us/sample - loss: 0.1013 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "170/170 [==============================] - 0s 162us/sample - loss: 0.0987 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "170/170 [==============================] - 0s 155us/sample - loss: 0.0981 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "170/170 [==============================] - 0s 148us/sample - loss: 0.0976 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "170/170 [==============================] - 0s 188us/sample - loss: 0.0943 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "170/170 [==============================] - 0s 157us/sample - loss: 0.0923 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "170/170 [==============================] - 0s 157us/sample - loss: 0.0926 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "170/170 [==============================] - 0s 162us/sample - loss: 0.0867 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "170/170 [==============================] - 0s 166us/sample - loss: 0.0843 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "170/170 [==============================] - 0s 160us/sample - loss: 0.0838 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "170/170 [==============================] - 0s 158us/sample - loss: 0.0826 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "170/170 [==============================] - 0s 155us/sample - loss: 0.0824 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "170/170 [==============================] - 0s 190us/sample - loss: 0.0833 - accuracy: 0.9941\n",
            "Epoch 161/200\n",
            "170/170 [==============================] - 0s 154us/sample - loss: 0.0794 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "170/170 [==============================] - 0s 157us/sample - loss: 0.0769 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "170/170 [==============================] - 0s 172us/sample - loss: 0.0746 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "170/170 [==============================] - 0s 159us/sample - loss: 0.0728 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "170/170 [==============================] - 0s 155us/sample - loss: 0.0709 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "170/170 [==============================] - 0s 153us/sample - loss: 0.0697 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "170/170 [==============================] - 0s 188us/sample - loss: 0.0682 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "170/170 [==============================] - 0s 230us/sample - loss: 0.0671 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "170/170 [==============================] - 0s 168us/sample - loss: 0.0657 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "170/170 [==============================] - 0s 177us/sample - loss: 0.0655 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "170/170 [==============================] - 0s 167us/sample - loss: 0.0640 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "170/170 [==============================] - 0s 187us/sample - loss: 0.0629 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "170/170 [==============================] - 0s 169us/sample - loss: 0.0615 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "170/170 [==============================] - 0s 145us/sample - loss: 0.0609 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "170/170 [==============================] - 0s 149us/sample - loss: 0.0598 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "170/170 [==============================] - 0s 173us/sample - loss: 0.0591 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "170/170 [==============================] - 0s 172us/sample - loss: 0.0598 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "170/170 [==============================] - 0s 163us/sample - loss: 0.0593 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "170/170 [==============================] - 0s 150us/sample - loss: 0.0579 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "170/170 [==============================] - 0s 160us/sample - loss: 0.0573 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "170/170 [==============================] - 0s 174us/sample - loss: 0.0536 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "170/170 [==============================] - 0s 164us/sample - loss: 0.0530 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "170/170 [==============================] - 0s 154us/sample - loss: 0.0517 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "170/170 [==============================] - 0s 191us/sample - loss: 0.0514 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "170/170 [==============================] - 0s 152us/sample - loss: 0.0508 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "170/170 [==============================] - 0s 154us/sample - loss: 0.0495 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "170/170 [==============================] - 0s 173us/sample - loss: 0.0494 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "170/170 [==============================] - 0s 162us/sample - loss: 0.0480 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "170/170 [==============================] - 0s 160us/sample - loss: 0.0469 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "170/170 [==============================] - 0s 143us/sample - loss: 0.0471 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "170/170 [==============================] - 0s 169us/sample - loss: 0.0458 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "170/170 [==============================] - 0s 173us/sample - loss: 0.0461 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "170/170 [==============================] - 0s 162us/sample - loss: 0.0448 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "170/170 [==============================] - 0s 165us/sample - loss: 0.0439 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "170/170 [==============================] - 0s 151us/sample - loss: 0.0429 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "170/170 [==============================] - 0s 160us/sample - loss: 0.0421 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "170/170 [==============================] - 0s 147us/sample - loss: 0.0416 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "170/170 [==============================] - 0s 170us/sample - loss: 0.0411 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "170/170 [==============================] - 0s 155us/sample - loss: 0.0407 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "170/170 [==============================] - 0s 168us/sample - loss: 0.0402 - accuracy: 1.0000\n",
            "CPU times: user 8.89 s, sys: 1.06 s, total: 9.95 s\n",
            "Wall time: 8.74 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb9817b6978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7Bf90ATg98K",
        "colab_type": "text"
      },
      "source": [
        "## **Generate Text:** Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uBnrJJe0fgz",
        "colab_type": "text"
      },
      "source": [
        "### Define generate_seq function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFq9I4aCDI6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, char_dic, seq_length, seed_text, n_chars):\n",
        "\tin_text = seed_text\t\t\t\t\t\t\t\t\t# seed text + generated text\n",
        "\t# generate a fixed number of characters\n",
        "\tfor i in range(n_chars):\n",
        "\t\t# encode the characters as integers\n",
        "\t\tencoded = [char_dic[char] for char in in_text]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# one hot encode\n",
        "\t\tencoded = tf.keras.utils.to_categorical(encoded, num_classes=len(char_dic))\n",
        "\t\t#encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
        "\t\t# predict character\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# reverse map integer to character\n",
        "\t\tfor char, index in char_dic.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += char\n",
        "\treturn in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2_tIRa7h1PUA"
      },
      "source": [
        "### Generate a sequence of characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhgIpBwG06uc",
        "colab_type": "text"
      },
      "source": [
        "학습문장: if you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC3D8JPW1O0Y",
        "colab_type": "code",
        "outputId": "4e1b51ee-e766-418b-97a9-635f5b74c2ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "# test start of rhyme\n",
        "print('Trigger characters: \"want to bu \"\\nResult: \\n\"{}\"'.format( \n",
        "      generate_seq(model, char_dic, 10, 'want to bu', 200)))\n",
        "# test mid-line\n",
        "print('\\nTrigger characters: \"collect wo\"\\nResult: \\n\"{}\"'.format(\n",
        "      generate_seq(model, char_dic, 10, 'collect wo', 100)))\n",
        "# test not in original\n",
        "print('\\nTrigger characters: \"rather than\"\\nResult: \"{}\"'.format(\n",
        "      generate_seq(model, char_dic, 10, 'rather than', 200)))\n",
        "# test end of rhyme\n",
        "print('Trigger characters: \"immensity of the sea.\"\\nResult: \\n\"{}\"'.format( \n",
        "      generate_seq(model, char_dic, 10, 'immensity of the sea.', 360)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trigger characters: \"want to bu \"\n",
            "Result: \n",
            "\"want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.assniishe mt mtmgg 'fo ottee  d  d sm\"\n",
            "\n",
            "Trigger characters: \"collect wo\"\n",
            "Result: \n",
            "\"collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of \"\n",
            "\n",
            "Trigger characters: \"rather than\"\n",
            "Result: \"rather thanh thes aaklsntir hhhra ctclllletooo rrr  ldleaceestt iht hhe so llgggheetettccs i  oooo o dooddnn'd oogspp tteeeeeees siiiitnit    uunnss  t kke,,    hhhhee  googg tteoooeec   wwoim   ddoond'  soong t\"\n",
            "Trigger characters: \"immensity of the sea.\"\n",
            "Result: \n",
            "\"immensity of the sea.assniishe mt mtmgg 'fo ottee  d  d smme' syyy f ttttoeead    aimn,     bbb,e o hhiheetoo  ttt  ra.al leaahhhehm tao oggddrrettottttcl d dddd  aasspp,d enndnda mmmkpppeeyftttoet  ld   sssee ne iisiii tt   d hassm ''ttte  a  a ssh ee nmyy  iiheggee tttttem sld  ssnf,, d drbbbut ttor  aollleeethh r smsslleeeyttsh mis l.sygeftettttth d ddlll aatsp hheeeeeggg tii\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnaUZbi_hIHw",
        "colab_type": "text"
      },
      "source": [
        "## **실습과제**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07YM3eLIWxSY",
        "colab_type": "text"
      },
      "source": [
        "### 과제 1. 더 긴문장을 생성하도록 해보자\n",
        "####-- 학습문장이 \" ...  immensity of the sea.\"로 끝났다. 첫번째 생성문장이 뒷부분이 이상하다. 180자로 학습한 한계 때문일까? \n",
        "#### -- 학습문장 전체를 한번더 반복하여 학습문장을 360자로 확장하고 결과를 비교해 보자\n",
        "#### -- 결과를 분석하자: \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flZbvaIEXuk7",
        "colab_type": "code",
        "outputId": "1a07d9e6-c41a-4d83-ac77-68bdee33fa10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "# test end of rhyme\n",
        "print('Trigger characters: \"immensity of the sea.\"\\nResult: \\n\"{}\"'.format( \n",
        "      generate_seq(model, char_dic, 10, 'immensity of the sea.', 360)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trigger characters: \"immensity of the sea.\"\n",
            "Result: \n",
            "\"immensity of the sea.assniishe mt mtmgg 'fo ottee  d  d smme' syyy f ttttoeead    aimn,     bbb,e o hhiheetoo  ttt  ra.al leaahhhehm tao oggddrrettottttcl d dddd  aasspp,d enndnda mmmkpppeeyftttoet  ld   sssee ne iisiii tt   d hassm ''ttte  a  a ssh ee nmyy  iiheggee tttttem sld  ssnf,, d drbbbut ttor  aollleeethh r smsslleeeyttsh mis l.sygeftettttth d ddlll aatsp hheeeeeggg tii\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uqymQOfGZW2",
        "colab_type": "text"
      },
      "source": [
        "### 과제 2. 세번째 생성문장은 여전히 문제가 많다. 모델을 개선해 보자 \n",
        "####-- LSTM layer를 추가해 보자 \n",
        "####-- 더 무엇을 해볼까? 생각해 보자\n",
        "####-- 결과를 분석하자:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AqJ_sPuyYzzi"
      },
      "source": [
        "## **Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EJz5eFI7Yzzl"
      },
      "source": [
        "### Define"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d6bb3396-d475-4c29-e7a5-d051ff1d1d73",
        "id": "ALfDg75GYzzm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "# Sequenctial model define: LSTM + Dense\n",
        "hidden_dim = 75\n",
        "model_2 = tf.keras.models.Sequential()\n",
        "model_2.add(tf.keras.layers.LSTM(units=hidden_dim,\n",
        "                               input_shape=(sequence_length,num_classes), return_sequences=True))\n",
        "model_2.add(tf.keras.layers.LSTM(units=hidden_dim, return_sequences=True))\n",
        "model_2.add(tf.keras.layers.LSTM(units=hidden_dim))\n",
        "model_2.add(tf.keras.layers.Dense(data_dim, activation='softmax'))\n",
        "\n",
        "model_2.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_7 (LSTM)                (None, 10, 75)            30300     \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 10, 75)            45300     \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 75)                45300     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 25)                1900      \n",
            "=================================================================\n",
            "Total params: 122,800\n",
            "Trainable params: 122,800\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5aKLh5ZQYzzr"
      },
      "source": [
        "### Compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6XXS-JjwYzzs",
        "colab": {}
      },
      "source": [
        "model_2.compile(loss='categorical_crossentropy', # one-hot coding\n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oJsJF-H1Yzzt"
      },
      "source": [
        "### Fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y0849zEHYzzu"
      },
      "source": [
        "Epoch 1000/1000   \n",
        "170/170 [==============================] - 0s 235us/sample - loss: 0.0012 - accuracy: 1.0000    \n",
        "CPU times: user 48.3 s, sys: 6.54 s, total: 54.8 s    \n",
        "Wall time: 47.8 s (@Notebook Setting/GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e3039198-2b6c-4ed5-d315-b465ee8f88f4",
        "id": "RWfONZX6Yzzv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "model_2.fit(X,            # X.shape : (170, 10, 25)\n",
        "            y,            # y.shape : (170, 25)\n",
        "            epochs=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 170 samples\n",
            "Epoch 1/200\n",
            "170/170 [==============================] - 4s 22ms/sample - loss: 3.2048 - accuracy: 0.1765\n",
            "Epoch 2/200\n",
            "170/170 [==============================] - 0s 292us/sample - loss: 3.1398 - accuracy: 0.1882\n",
            "Epoch 3/200\n",
            "170/170 [==============================] - 0s 262us/sample - loss: 2.9726 - accuracy: 0.1882\n",
            "Epoch 4/200\n",
            "170/170 [==============================] - 0s 264us/sample - loss: 2.9003 - accuracy: 0.1882\n",
            "Epoch 5/200\n",
            "170/170 [==============================] - 0s 268us/sample - loss: 2.8716 - accuracy: 0.1882\n",
            "Epoch 6/200\n",
            "170/170 [==============================] - 0s 257us/sample - loss: 2.8492 - accuracy: 0.1882\n",
            "Epoch 7/200\n",
            "170/170 [==============================] - 0s 286us/sample - loss: 2.8368 - accuracy: 0.1882\n",
            "Epoch 8/200\n",
            "170/170 [==============================] - 0s 334us/sample - loss: 2.8326 - accuracy: 0.1882\n",
            "Epoch 9/200\n",
            "170/170 [==============================] - 0s 324us/sample - loss: 2.8225 - accuracy: 0.1882\n",
            "Epoch 10/200\n",
            "170/170 [==============================] - 0s 322us/sample - loss: 2.8251 - accuracy: 0.1882\n",
            "Epoch 11/200\n",
            "170/170 [==============================] - 0s 261us/sample - loss: 2.8094 - accuracy: 0.1882\n",
            "Epoch 12/200\n",
            "170/170 [==============================] - 0s 267us/sample - loss: 2.8084 - accuracy: 0.1882\n",
            "Epoch 13/200\n",
            "170/170 [==============================] - 0s 303us/sample - loss: 2.7918 - accuracy: 0.1882\n",
            "Epoch 14/200\n",
            "170/170 [==============================] - 0s 314us/sample - loss: 2.7874 - accuracy: 0.1882\n",
            "Epoch 15/200\n",
            "170/170 [==============================] - 0s 274us/sample - loss: 2.7711 - accuracy: 0.1882\n",
            "Epoch 16/200\n",
            "170/170 [==============================] - 0s 285us/sample - loss: 2.7605 - accuracy: 0.1882\n",
            "Epoch 17/200\n",
            "170/170 [==============================] - 0s 272us/sample - loss: 2.7441 - accuracy: 0.2000\n",
            "Epoch 18/200\n",
            "170/170 [==============================] - 0s 259us/sample - loss: 2.7191 - accuracy: 0.1941\n",
            "Epoch 19/200\n",
            "170/170 [==============================] - 0s 245us/sample - loss: 2.6916 - accuracy: 0.1882\n",
            "Epoch 20/200\n",
            "170/170 [==============================] - 0s 274us/sample - loss: 2.6432 - accuracy: 0.2000\n",
            "Epoch 21/200\n",
            "170/170 [==============================] - 0s 291us/sample - loss: 2.6123 - accuracy: 0.2000\n",
            "Epoch 22/200\n",
            "170/170 [==============================] - 0s 254us/sample - loss: 2.5870 - accuracy: 0.2000\n",
            "Epoch 23/200\n",
            "170/170 [==============================] - 0s 272us/sample - loss: 2.5607 - accuracy: 0.2118\n",
            "Epoch 24/200\n",
            "170/170 [==============================] - 0s 269us/sample - loss: 2.4905 - accuracy: 0.2471\n",
            "Epoch 25/200\n",
            "170/170 [==============================] - 0s 274us/sample - loss: 2.4606 - accuracy: 0.2176\n",
            "Epoch 26/200\n",
            "170/170 [==============================] - 0s 260us/sample - loss: 2.4400 - accuracy: 0.2294\n",
            "Epoch 27/200\n",
            "170/170 [==============================] - 0s 269us/sample - loss: 2.3931 - accuracy: 0.2353\n",
            "Epoch 28/200\n",
            "170/170 [==============================] - 0s 297us/sample - loss: 2.3924 - accuracy: 0.2235\n",
            "Epoch 29/200\n",
            "170/170 [==============================] - 0s 337us/sample - loss: 2.3344 - accuracy: 0.2118\n",
            "Epoch 30/200\n",
            "170/170 [==============================] - 0s 256us/sample - loss: 2.2996 - accuracy: 0.2647\n",
            "Epoch 31/200\n",
            "170/170 [==============================] - 0s 261us/sample - loss: 2.2463 - accuracy: 0.2706\n",
            "Epoch 32/200\n",
            "170/170 [==============================] - 0s 274us/sample - loss: 2.1910 - accuracy: 0.2882\n",
            "Epoch 33/200\n",
            "170/170 [==============================] - 0s 277us/sample - loss: 2.1557 - accuracy: 0.3176\n",
            "Epoch 34/200\n",
            "170/170 [==============================] - 0s 267us/sample - loss: 2.0748 - accuracy: 0.3176\n",
            "Epoch 35/200\n",
            "170/170 [==============================] - 0s 267us/sample - loss: 2.0509 - accuracy: 0.3824\n",
            "Epoch 36/200\n",
            "170/170 [==============================] - 0s 265us/sample - loss: 1.9713 - accuracy: 0.3588\n",
            "Epoch 37/200\n",
            "170/170 [==============================] - 0s 264us/sample - loss: 1.8995 - accuracy: 0.4176\n",
            "Epoch 38/200\n",
            "170/170 [==============================] - 0s 278us/sample - loss: 1.8878 - accuracy: 0.4176\n",
            "Epoch 39/200\n",
            "170/170 [==============================] - 0s 264us/sample - loss: 1.7902 - accuracy: 0.4647\n",
            "Epoch 40/200\n",
            "170/170 [==============================] - 0s 287us/sample - loss: 1.7504 - accuracy: 0.4706\n",
            "Epoch 41/200\n",
            "170/170 [==============================] - 0s 320us/sample - loss: 1.6840 - accuracy: 0.4412\n",
            "Epoch 42/200\n",
            "170/170 [==============================] - 0s 279us/sample - loss: 1.6429 - accuracy: 0.4941\n",
            "Epoch 43/200\n",
            "170/170 [==============================] - 0s 305us/sample - loss: 1.6256 - accuracy: 0.4824\n",
            "Epoch 44/200\n",
            "170/170 [==============================] - 0s 274us/sample - loss: 1.5969 - accuracy: 0.4824\n",
            "Epoch 45/200\n",
            "170/170 [==============================] - 0s 262us/sample - loss: 1.5439 - accuracy: 0.5000\n",
            "Epoch 46/200\n",
            "170/170 [==============================] - 0s 267us/sample - loss: 1.5717 - accuracy: 0.4824\n",
            "Epoch 47/200\n",
            "170/170 [==============================] - 0s 300us/sample - loss: 1.4325 - accuracy: 0.5824\n",
            "Epoch 48/200\n",
            "170/170 [==============================] - 0s 305us/sample - loss: 1.4173 - accuracy: 0.5471\n",
            "Epoch 49/200\n",
            "170/170 [==============================] - 0s 369us/sample - loss: 1.4027 - accuracy: 0.5941\n",
            "Epoch 50/200\n",
            "170/170 [==============================] - 0s 283us/sample - loss: 1.3404 - accuracy: 0.5824\n",
            "Epoch 51/200\n",
            "170/170 [==============================] - 0s 281us/sample - loss: 1.3110 - accuracy: 0.6353\n",
            "Epoch 52/200\n",
            "170/170 [==============================] - 0s 270us/sample - loss: 1.2484 - accuracy: 0.6118\n",
            "Epoch 53/200\n",
            "170/170 [==============================] - 0s 271us/sample - loss: 1.2614 - accuracy: 0.6588\n",
            "Epoch 54/200\n",
            "170/170 [==============================] - 0s 270us/sample - loss: 1.2311 - accuracy: 0.6765\n",
            "Epoch 55/200\n",
            "170/170 [==============================] - 0s 279us/sample - loss: 1.1720 - accuracy: 0.6235\n",
            "Epoch 56/200\n",
            "170/170 [==============================] - 0s 299us/sample - loss: 1.1861 - accuracy: 0.6529\n",
            "Epoch 57/200\n",
            "170/170 [==============================] - 0s 320us/sample - loss: 1.1484 - accuracy: 0.6529\n",
            "Epoch 58/200\n",
            "170/170 [==============================] - 0s 267us/sample - loss: 1.1233 - accuracy: 0.7059\n",
            "Epoch 59/200\n",
            "170/170 [==============================] - 0s 266us/sample - loss: 1.0765 - accuracy: 0.6765\n",
            "Epoch 60/200\n",
            "170/170 [==============================] - 0s 286us/sample - loss: 1.0269 - accuracy: 0.7412\n",
            "Epoch 61/200\n",
            "170/170 [==============================] - 0s 276us/sample - loss: 0.9553 - accuracy: 0.7647\n",
            "Epoch 62/200\n",
            "170/170 [==============================] - 0s 273us/sample - loss: 0.8780 - accuracy: 0.8118\n",
            "Epoch 63/200\n",
            "170/170 [==============================] - 0s 276us/sample - loss: 0.8538 - accuracy: 0.8118\n",
            "Epoch 64/200\n",
            "170/170 [==============================] - 0s 302us/sample - loss: 0.8088 - accuracy: 0.8235\n",
            "Epoch 65/200\n",
            "170/170 [==============================] - 0s 320us/sample - loss: 0.7965 - accuracy: 0.8235\n",
            "Epoch 66/200\n",
            "170/170 [==============================] - 0s 271us/sample - loss: 0.7610 - accuracy: 0.8412\n",
            "Epoch 67/200\n",
            "170/170 [==============================] - 0s 279us/sample - loss: 0.7284 - accuracy: 0.8588\n",
            "Epoch 68/200\n",
            "170/170 [==============================] - 0s 313us/sample - loss: 0.7070 - accuracy: 0.8647\n",
            "Epoch 69/200\n",
            "170/170 [==============================] - 0s 290us/sample - loss: 0.6771 - accuracy: 0.8765\n",
            "Epoch 70/200\n",
            "170/170 [==============================] - 0s 328us/sample - loss: 0.6353 - accuracy: 0.8824\n",
            "Epoch 71/200\n",
            "170/170 [==============================] - 0s 272us/sample - loss: 0.6075 - accuracy: 0.9059\n",
            "Epoch 72/200\n",
            "170/170 [==============================] - 0s 256us/sample - loss: 0.5865 - accuracy: 0.8882\n",
            "Epoch 73/200\n",
            "170/170 [==============================] - 0s 296us/sample - loss: 0.5594 - accuracy: 0.9294\n",
            "Epoch 74/200\n",
            "170/170 [==============================] - 0s 272us/sample - loss: 0.5542 - accuracy: 0.9412\n",
            "Epoch 75/200\n",
            "170/170 [==============================] - 0s 316us/sample - loss: 0.5260 - accuracy: 0.9176\n",
            "Epoch 76/200\n",
            "170/170 [==============================] - 0s 271us/sample - loss: 0.4894 - accuracy: 0.9294\n",
            "Epoch 77/200\n",
            "170/170 [==============================] - 0s 312us/sample - loss: 0.4747 - accuracy: 0.9353\n",
            "Epoch 78/200\n",
            "170/170 [==============================] - 0s 277us/sample - loss: 0.4537 - accuracy: 0.9412\n",
            "Epoch 79/200\n",
            "170/170 [==============================] - 0s 261us/sample - loss: 0.4454 - accuracy: 0.9412\n",
            "Epoch 80/200\n",
            "170/170 [==============================] - 0s 274us/sample - loss: 0.4399 - accuracy: 0.9235\n",
            "Epoch 81/200\n",
            "170/170 [==============================] - 0s 289us/sample - loss: 0.4231 - accuracy: 0.9353\n",
            "Epoch 82/200\n",
            "170/170 [==============================] - 0s 275us/sample - loss: 0.4119 - accuracy: 0.9471\n",
            "Epoch 83/200\n",
            "170/170 [==============================] - 0s 317us/sample - loss: 0.4548 - accuracy: 0.9176\n",
            "Epoch 84/200\n",
            "170/170 [==============================] - 0s 278us/sample - loss: 0.3966 - accuracy: 0.9412\n",
            "Epoch 85/200\n",
            "170/170 [==============================] - 0s 268us/sample - loss: 0.4026 - accuracy: 0.9353\n",
            "Epoch 86/200\n",
            "170/170 [==============================] - 0s 304us/sample - loss: 0.3524 - accuracy: 0.9529\n",
            "Epoch 87/200\n",
            "170/170 [==============================] - 0s 262us/sample - loss: 0.3378 - accuracy: 0.9706\n",
            "Epoch 88/200\n",
            "170/170 [==============================] - 0s 268us/sample - loss: 0.3285 - accuracy: 0.9706\n",
            "Epoch 89/200\n",
            "170/170 [==============================] - 0s 342us/sample - loss: 0.3514 - accuracy: 0.9588\n",
            "Epoch 90/200\n",
            "170/170 [==============================] - 0s 295us/sample - loss: 0.3346 - accuracy: 0.9706\n",
            "Epoch 91/200\n",
            "170/170 [==============================] - 0s 313us/sample - loss: 0.3259 - accuracy: 0.9824\n",
            "Epoch 92/200\n",
            "170/170 [==============================] - 0s 258us/sample - loss: 0.3027 - accuracy: 0.9765\n",
            "Epoch 93/200\n",
            "170/170 [==============================] - 0s 262us/sample - loss: 0.3012 - accuracy: 0.9588\n",
            "Epoch 94/200\n",
            "170/170 [==============================] - 0s 262us/sample - loss: 0.3280 - accuracy: 0.9529\n",
            "Epoch 95/200\n",
            "170/170 [==============================] - 0s 274us/sample - loss: 0.3351 - accuracy: 0.9471\n",
            "Epoch 96/200\n",
            "170/170 [==============================] - 0s 263us/sample - loss: 0.2955 - accuracy: 0.9882\n",
            "Epoch 97/200\n",
            "170/170 [==============================] - 0s 259us/sample - loss: 0.2549 - accuracy: 0.9882\n",
            "Epoch 98/200\n",
            "170/170 [==============================] - 0s 265us/sample - loss: 0.2653 - accuracy: 0.9706\n",
            "Epoch 99/200\n",
            "170/170 [==============================] - 0s 267us/sample - loss: 0.2572 - accuracy: 0.9765\n",
            "Epoch 100/200\n",
            "170/170 [==============================] - 0s 263us/sample - loss: 0.2262 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "170/170 [==============================] - 0s 272us/sample - loss: 0.2188 - accuracy: 0.9882\n",
            "Epoch 102/200\n",
            "170/170 [==============================] - 0s 284us/sample - loss: 0.2476 - accuracy: 0.9765\n",
            "Epoch 103/200\n",
            "170/170 [==============================] - 0s 265us/sample - loss: 0.2206 - accuracy: 0.9824\n",
            "Epoch 104/200\n",
            "170/170 [==============================] - 0s 270us/sample - loss: 0.2014 - accuracy: 0.9882\n",
            "Epoch 105/200\n",
            "170/170 [==============================] - 0s 256us/sample - loss: 0.1973 - accuracy: 0.9882\n",
            "Epoch 106/200\n",
            "170/170 [==============================] - 0s 254us/sample - loss: 0.1851 - accuracy: 0.9882\n",
            "Epoch 107/200\n",
            "170/170 [==============================] - 0s 271us/sample - loss: 0.1727 - accuracy: 0.9941\n",
            "Epoch 108/200\n",
            "170/170 [==============================] - 0s 311us/sample - loss: 0.1655 - accuracy: 0.9941\n",
            "Epoch 109/200\n",
            "170/170 [==============================] - 0s 279us/sample - loss: 0.1600 - accuracy: 0.9941\n",
            "Epoch 110/200\n",
            "170/170 [==============================] - 0s 270us/sample - loss: 0.1533 - accuracy: 0.9882\n",
            "Epoch 111/200\n",
            "170/170 [==============================] - 0s 295us/sample - loss: 0.1626 - accuracy: 0.9941\n",
            "Epoch 112/200\n",
            "170/170 [==============================] - 0s 333us/sample - loss: 0.1616 - accuracy: 0.9882\n",
            "Epoch 113/200\n",
            "170/170 [==============================] - 0s 286us/sample - loss: 0.1712 - accuracy: 0.9824\n",
            "Epoch 114/200\n",
            "170/170 [==============================] - 0s 262us/sample - loss: 0.1497 - accuracy: 0.9941\n",
            "Epoch 115/200\n",
            "170/170 [==============================] - 0s 274us/sample - loss: 0.1400 - accuracy: 0.9882\n",
            "Epoch 116/200\n",
            "170/170 [==============================] - 0s 299us/sample - loss: 0.1434 - accuracy: 0.9882\n",
            "Epoch 117/200\n",
            "170/170 [==============================] - 0s 273us/sample - loss: 0.1365 - accuracy: 0.9882\n",
            "Epoch 118/200\n",
            "170/170 [==============================] - 0s 261us/sample - loss: 0.1319 - accuracy: 0.9882\n",
            "Epoch 119/200\n",
            "170/170 [==============================] - 0s 264us/sample - loss: 0.1241 - accuracy: 0.9941\n",
            "Epoch 120/200\n",
            "170/170 [==============================] - 0s 252us/sample - loss: 0.1238 - accuracy: 0.9882\n",
            "Epoch 121/200\n",
            "170/170 [==============================] - 0s 252us/sample - loss: 0.2455 - accuracy: 0.9471\n",
            "Epoch 122/200\n",
            "170/170 [==============================] - 0s 257us/sample - loss: 0.4062 - accuracy: 0.9059\n",
            "Epoch 123/200\n",
            "170/170 [==============================] - 0s 291us/sample - loss: 0.3790 - accuracy: 0.8824\n",
            "Epoch 124/200\n",
            "170/170 [==============================] - 0s 335us/sample - loss: 0.2924 - accuracy: 0.9412\n",
            "Epoch 125/200\n",
            "170/170 [==============================] - 0s 293us/sample - loss: 0.2303 - accuracy: 0.9824\n",
            "Epoch 126/200\n",
            "170/170 [==============================] - 0s 288us/sample - loss: 0.2567 - accuracy: 0.9647\n",
            "Epoch 127/200\n",
            "170/170 [==============================] - 0s 277us/sample - loss: 0.2661 - accuracy: 0.9647\n",
            "Epoch 128/200\n",
            "170/170 [==============================] - 0s 259us/sample - loss: 0.2244 - accuracy: 0.9824\n",
            "Epoch 129/200\n",
            "170/170 [==============================] - 0s 267us/sample - loss: 0.1977 - accuracy: 0.9824\n",
            "Epoch 130/200\n",
            "170/170 [==============================] - 0s 290us/sample - loss: 0.1590 - accuracy: 0.9765\n",
            "Epoch 131/200\n",
            "170/170 [==============================] - 0s 283us/sample - loss: 0.1483 - accuracy: 0.9824\n",
            "Epoch 132/200\n",
            "170/170 [==============================] - 0s 291us/sample - loss: 0.1272 - accuracy: 0.9882\n",
            "Epoch 133/200\n",
            "170/170 [==============================] - 0s 341us/sample - loss: 0.1147 - accuracy: 0.9941\n",
            "Epoch 134/200\n",
            "170/170 [==============================] - 0s 273us/sample - loss: 0.1073 - accuracy: 0.9941\n",
            "Epoch 135/200\n",
            "170/170 [==============================] - 0s 327us/sample - loss: 0.1008 - accuracy: 0.9941\n",
            "Epoch 136/200\n",
            "170/170 [==============================] - 0s 255us/sample - loss: 0.0883 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "170/170 [==============================] - 0s 302us/sample - loss: 0.0874 - accuracy: 0.9941\n",
            "Epoch 138/200\n",
            "170/170 [==============================] - 0s 281us/sample - loss: 0.0822 - accuracy: 0.9941\n",
            "Epoch 139/200\n",
            "170/170 [==============================] - 0s 282us/sample - loss: 0.0766 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "170/170 [==============================] - 0s 257us/sample - loss: 0.0784 - accuracy: 0.9941\n",
            "Epoch 141/200\n",
            "170/170 [==============================] - 0s 275us/sample - loss: 0.1007 - accuracy: 0.9882\n",
            "Epoch 142/200\n",
            "170/170 [==============================] - 0s 273us/sample - loss: 0.0947 - accuracy: 0.9941\n",
            "Epoch 143/200\n",
            "170/170 [==============================] - 0s 260us/sample - loss: 0.0877 - accuracy: 0.9941\n",
            "Epoch 144/200\n",
            "170/170 [==============================] - 0s 255us/sample - loss: 0.0865 - accuracy: 0.9941\n",
            "Epoch 145/200\n",
            "170/170 [==============================] - 0s 261us/sample - loss: 0.0760 - accuracy: 0.9941\n",
            "Epoch 146/200\n",
            "170/170 [==============================] - 0s 266us/sample - loss: 0.0741 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "170/170 [==============================] - 0s 396us/sample - loss: 0.0684 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "170/170 [==============================] - 0s 273us/sample - loss: 0.0689 - accuracy: 0.9941\n",
            "Epoch 149/200\n",
            "170/170 [==============================] - 0s 272us/sample - loss: 0.0732 - accuracy: 0.9882\n",
            "Epoch 150/200\n",
            "170/170 [==============================] - 0s 261us/sample - loss: 0.0693 - accuracy: 0.9941\n",
            "Epoch 151/200\n",
            "170/170 [==============================] - 0s 361us/sample - loss: 0.0661 - accuracy: 0.9941\n",
            "Epoch 152/200\n",
            "170/170 [==============================] - 0s 264us/sample - loss: 0.0647 - accuracy: 0.9941\n",
            "Epoch 153/200\n",
            "170/170 [==============================] - 0s 294us/sample - loss: 0.0879 - accuracy: 0.9882\n",
            "Epoch 154/200\n",
            "170/170 [==============================] - 0s 308us/sample - loss: 0.0785 - accuracy: 0.9882\n",
            "Epoch 155/200\n",
            "170/170 [==============================] - 0s 268us/sample - loss: 0.0748 - accuracy: 0.9941\n",
            "Epoch 156/200\n",
            "170/170 [==============================] - 0s 275us/sample - loss: 0.0697 - accuracy: 0.9882\n",
            "Epoch 157/200\n",
            "170/170 [==============================] - 0s 265us/sample - loss: 0.0640 - accuracy: 0.9941\n",
            "Epoch 158/200\n",
            "170/170 [==============================] - 0s 265us/sample - loss: 0.0609 - accuracy: 0.9941\n",
            "Epoch 159/200\n",
            "170/170 [==============================] - 0s 256us/sample - loss: 0.0580 - accuracy: 0.9941\n",
            "Epoch 160/200\n",
            "170/170 [==============================] - 0s 252us/sample - loss: 0.0566 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "170/170 [==============================] - 0s 257us/sample - loss: 0.0559 - accuracy: 0.9941\n",
            "Epoch 162/200\n",
            "170/170 [==============================] - 0s 267us/sample - loss: 0.0545 - accuracy: 0.9941\n",
            "Epoch 163/200\n",
            "170/170 [==============================] - 0s 264us/sample - loss: 0.0550 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "170/170 [==============================] - 0s 265us/sample - loss: 0.0531 - accuracy: 0.9941\n",
            "Epoch 165/200\n",
            "170/170 [==============================] - 0s 254us/sample - loss: 0.0510 - accuracy: 0.9941\n",
            "Epoch 166/200\n",
            "170/170 [==============================] - 0s 273us/sample - loss: 0.0499 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "170/170 [==============================] - 0s 261us/sample - loss: 0.0491 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "170/170 [==============================] - 0s 266us/sample - loss: 0.0468 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "170/170 [==============================] - 0s 270us/sample - loss: 0.0449 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "170/170 [==============================] - 0s 267us/sample - loss: 0.0448 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "170/170 [==============================] - 0s 263us/sample - loss: 0.0567 - accuracy: 0.9941\n",
            "Epoch 172/200\n",
            "170/170 [==============================] - 0s 268us/sample - loss: 0.0563 - accuracy: 0.9941\n",
            "Epoch 173/200\n",
            "170/170 [==============================] - 0s 278us/sample - loss: 0.0527 - accuracy: 0.9941\n",
            "Epoch 174/200\n",
            "170/170 [==============================] - 0s 316us/sample - loss: 0.0476 - accuracy: 0.9941\n",
            "Epoch 175/200\n",
            "170/170 [==============================] - 0s 300us/sample - loss: 0.0452 - accuracy: 0.9941\n",
            "Epoch 176/200\n",
            "170/170 [==============================] - 0s 330us/sample - loss: 0.0425 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "170/170 [==============================] - 0s 277us/sample - loss: 0.0416 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "170/170 [==============================] - 0s 265us/sample - loss: 0.0400 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "170/170 [==============================] - 0s 272us/sample - loss: 0.0393 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "170/170 [==============================] - 0s 267us/sample - loss: 0.0384 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "170/170 [==============================] - 0s 258us/sample - loss: 0.0388 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "170/170 [==============================] - 0s 280us/sample - loss: 0.0566 - accuracy: 0.9941\n",
            "Epoch 183/200\n",
            "170/170 [==============================] - 0s 275us/sample - loss: 0.0411 - accuracy: 0.9941\n",
            "Epoch 184/200\n",
            "170/170 [==============================] - 0s 261us/sample - loss: 0.0433 - accuracy: 0.9941\n",
            "Epoch 185/200\n",
            "170/170 [==============================] - 0s 278us/sample - loss: 0.0388 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "170/170 [==============================] - 0s 336us/sample - loss: 0.0368 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "170/170 [==============================] - 0s 338us/sample - loss: 0.0361 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "170/170 [==============================] - 0s 324us/sample - loss: 0.0354 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "170/170 [==============================] - 0s 285us/sample - loss: 0.0339 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "170/170 [==============================] - 0s 268us/sample - loss: 0.0331 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "170/170 [==============================] - 0s 276us/sample - loss: 0.0322 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "170/170 [==============================] - 0s 300us/sample - loss: 0.0312 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "170/170 [==============================] - 0s 289us/sample - loss: 0.0306 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "170/170 [==============================] - 0s 329us/sample - loss: 0.0319 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "170/170 [==============================] - 0s 285us/sample - loss: 0.0555 - accuracy: 0.9882\n",
            "Epoch 196/200\n",
            "170/170 [==============================] - 0s 367us/sample - loss: 0.0444 - accuracy: 0.9941\n",
            "Epoch 197/200\n",
            "170/170 [==============================] - 0s 314us/sample - loss: 0.0375 - accuracy: 0.9941\n",
            "Epoch 198/200\n",
            "170/170 [==============================] - 0s 284us/sample - loss: 0.0333 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "170/170 [==============================] - 0s 303us/sample - loss: 0.0313 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "170/170 [==============================] - 0s 308us/sample - loss: 0.0296 - accuracy: 1.0000\n",
            "CPU times: user 15.1 s, sys: 1.05 s, total: 16.2 s\n",
            "Wall time: 13.5 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb951c3b5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4eEanWqZESr",
        "colab_type": "code",
        "outputId": "c87e98fa-a31f-4e7f-c08a-d3bbf0773378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "# test end of rhyme\n",
        "print('Trigger characters: \"immensity of the sea.\"\\nResult: \\n\"{}\"'.format( \n",
        "      generate_seq(model_2, char_dic, 10, 'immensity of the sea.', 200)))\n",
        "# test start of rhyme\n",
        "print('Trigger characters: \"want to bu \"\\nResult: \\n\"{}\"'.format( \n",
        "      generate_seq(model_2, char_dic, 10, 'want to bu', 200)))\n",
        "# test mid-line\n",
        "print('\\nTrigger characters: \"collect wo\"\\nResult: \\n\"{}\"'.format(\n",
        "      generate_seq(model_2, char_dic, 10, 'collect wo', 100)))\n",
        "# test not in original\n",
        "print('\\nTrigger characters: \"rather than\"\\nResult: \"{}\"'.format(\n",
        "      generate_seq(model_2, char_dic, 10, 'rather than', 200)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trigger characters: \"immensity of the sea.\"\n",
            "Result: \n",
            "\"immensity of the sea. ssii tth  tt  o o.sa aa. tdmww,b mm ouuuemffotlosst tofdtttt o  m     attat  r  lppheeeeetee. iiii  iitt   tta   arr d h eaccea ssimogttn'tnfoosmm   dgoedf  ts cntd  tat  b dr dharh eecdoccesi dfnog \"\n",
            "Trigger characters: \"want to bu \"\n",
            "Result: \n",
            "\"want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea. ssii tth  tt  o o.sa aa. tdmww,b mm \"\n",
            "\n",
            "Trigger characters: \"collect wo\"\n",
            "Result: \n",
            "\"collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of \"\n",
            "\n",
            "Trigger characters: \"rather than\"\n",
            "Result: \"rather thanhlteesn l stiiiehotty   ao   s ooona  't nn  hsm ut e .ghosss  cc .to''.l os    ttttt   s     a     ooooo  ttwwww ta  uuuuu   ttttttt o       taa    rr  he  eece   i ii s  rttt e neep  siiit ee tt   m\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2yG692dZBk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data를 늘려도, Layer를 더 늘려도 처음과 결과가 거의 비슷한 것을 볼 수 있다!!\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}